{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import *\n",
    "import numpy as np\n",
    "import textmining\n",
    "import re\n",
    "import lda\n",
    "import lda.datasets\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import nps_chat\n",
    "from scipy.sparse import coo_matrix\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopWords |= set(nps_chat.words())\n",
    "stopWords |= set(opinion_lexicon.negative())\n",
    "stopWords |= set(opinion_lexicon.positive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed all reviews!\n"
     ]
    }
   ],
   "source": [
    "reviewDocs = dict()\n",
    "with open(\"../input/review.json\") as json_file:\n",
    "    line_count = 0\n",
    "    for line in json_file:\n",
    "        review = json.loads(line)\n",
    "        text = review['text']\n",
    "        text = re.sub('[^A-Za-z ]+', '', text)\n",
    "        text = [word.strip() for word in text.lower().split() if word.strip() not in stopWords]\n",
    "#         text = ' '.join([stemmer.stem(word) for word in text.split() if stemmer.stem(word) not in stopWords])\n",
    "        reviewDocs[review['review_id']] = text\n",
    "    print(\"Processed all reviews!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4736897 2449413 83108547\n",
      "83108547 83108547 83108547\n"
     ]
    }
   ],
   "source": [
    "#Create Sparse Document-Term Matrix for LDA\n",
    "\n",
    "n_nonzero = 0\n",
    "vocab = set()\n",
    "for terms in reviewDocs.values():\n",
    "    unique_terms = set(terms)    # all unique terms of this doc\n",
    "    vocab |= unique_terms           # set union: add unique terms of this doc\n",
    "    n_nonzero += len(unique_terms)  # add count of unique terms in this doc\n",
    "\n",
    "# The ReviewId vector\n",
    "docnames = np.array(list(reviewDocs.keys()))\n",
    "# Create the vocab vector\n",
    "vocab = np.array(list(vocab)) \n",
    "# indices that sort vocab\n",
    "vocab_sorter = np.argsort(vocab)    \n",
    "\n",
    "ndocs = len(docnames)\n",
    "nvocab = len(vocab)\n",
    "print(ndocs,nvocab,n_nonzero)\n",
    "data = np.empty(n_nonzero, dtype=np.intc)     # unique terms in the combined corpus of all the document\n",
    "rows = np.empty(n_nonzero, dtype=np.intc)     # document index where the term is present\n",
    "cols = np.empty(n_nonzero, dtype=np.intc)     # position of the term in the document\n",
    "print(len(rows),len(cols),len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# current index in the sparse matrix data\n",
    "ind = 0\n",
    "# go through all reviews with their terms\n",
    "for docname, terms in reviewDocs.items():\n",
    "    # find indices into  such that, if the corresponding elements in were\n",
    "    # inserted before the indices, the order of  would be preserved\n",
    "    # -> array of indices of  in \n",
    "    term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter=vocab_sorter)]\n",
    "\n",
    "    # count the unique terms of the document and get their vocabulary indices\n",
    "    uniq_indices, counts = np.unique(term_indices, return_counts=True)\n",
    "    n_vals = len(uniq_indices)  # = number of unique terms\n",
    "    ind_end = ind + n_vals  #  to  is the slice that we will fill with data\n",
    "\n",
    "    data[ind:ind_end] = counts                  # save the counts (term frequencies)\n",
    "    cols[ind:ind_end] = uniq_indices            # save the column index: index in \n",
    "    doc_idx = np.where(docnames == docname)     # get the document index for the document name\n",
    "    rows[ind:ind_end] = np.repeat(doc_idx, n_vals)  # save it as repeated value\n",
    "\n",
    "    ind = ind_end  # resume with next document -> add data to the end\n",
    "    \n",
    "dtm = coo_matrix((data, (rows, cols)), shape=(ndocs, nvocab), dtype=np.intc)\n",
    "print(\"type(X): {}\".format(type(dtm)))\n",
    "print(\"shape: {}\".format(dtm.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 1000\n",
      "INFO:lda:vocab_size: 6177\n",
      "INFO:lda:n_words: 19258\n",
      "INFO:lda:n_topics: 50\n",
      "INFO:lda:n_iter: 2000\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -276401\n",
      "INFO:lda:<10> log likelihood: -184618\n",
      "INFO:lda:<20> log likelihood: -179267\n",
      "INFO:lda:<30> log likelihood: -176935\n",
      "INFO:lda:<40> log likelihood: -175910\n",
      "INFO:lda:<50> log likelihood: -174468\n",
      "INFO:lda:<60> log likelihood: -174546\n",
      "INFO:lda:<70> log likelihood: -173975\n",
      "INFO:lda:<80> log likelihood: -173571\n",
      "INFO:lda:<90> log likelihood: -173818\n",
      "INFO:lda:<100> log likelihood: -173444\n",
      "INFO:lda:<110> log likelihood: -173535\n",
      "INFO:lda:<120> log likelihood: -173356\n",
      "INFO:lda:<130> log likelihood: -172975\n",
      "INFO:lda:<140> log likelihood: -173182\n",
      "INFO:lda:<150> log likelihood: -172815\n",
      "INFO:lda:<160> log likelihood: -172623\n",
      "INFO:lda:<170> log likelihood: -172681\n",
      "INFO:lda:<180> log likelihood: -172795\n",
      "INFO:lda:<190> log likelihood: -172471\n",
      "INFO:lda:<200> log likelihood: -172721\n",
      "INFO:lda:<210> log likelihood: -172860\n",
      "INFO:lda:<220> log likelihood: -172746\n",
      "INFO:lda:<230> log likelihood: -172343\n",
      "INFO:lda:<240> log likelihood: -172695\n",
      "INFO:lda:<250> log likelihood: -172280\n",
      "INFO:lda:<260> log likelihood: -172398\n",
      "INFO:lda:<270> log likelihood: -172339\n",
      "INFO:lda:<280> log likelihood: -172467\n",
      "INFO:lda:<290> log likelihood: -172483\n",
      "INFO:lda:<300> log likelihood: -172112\n",
      "INFO:lda:<310> log likelihood: -172447\n",
      "INFO:lda:<320> log likelihood: -172646\n",
      "INFO:lda:<330> log likelihood: -172408\n",
      "INFO:lda:<340> log likelihood: -172245\n",
      "INFO:lda:<350> log likelihood: -171920\n",
      "INFO:lda:<360> log likelihood: -172210\n",
      "INFO:lda:<370> log likelihood: -172260\n",
      "INFO:lda:<380> log likelihood: -171866\n",
      "INFO:lda:<390> log likelihood: -171956\n",
      "INFO:lda:<400> log likelihood: -172374\n",
      "INFO:lda:<410> log likelihood: -172294\n",
      "INFO:lda:<420> log likelihood: -171952\n",
      "INFO:lda:<430> log likelihood: -172153\n",
      "INFO:lda:<440> log likelihood: -172093\n",
      "INFO:lda:<450> log likelihood: -171892\n",
      "INFO:lda:<460> log likelihood: -172125\n",
      "INFO:lda:<470> log likelihood: -172209\n",
      "INFO:lda:<480> log likelihood: -172321\n",
      "INFO:lda:<490> log likelihood: -171910\n",
      "INFO:lda:<500> log likelihood: -171796\n",
      "INFO:lda:<510> log likelihood: -172153\n",
      "INFO:lda:<520> log likelihood: -171857\n",
      "INFO:lda:<530> log likelihood: -172041\n",
      "INFO:lda:<540> log likelihood: -172269\n",
      "INFO:lda:<550> log likelihood: -172205\n",
      "INFO:lda:<560> log likelihood: -171871\n",
      "INFO:lda:<570> log likelihood: -171990\n",
      "INFO:lda:<580> log likelihood: -172256\n",
      "INFO:lda:<590> log likelihood: -172224\n",
      "INFO:lda:<600> log likelihood: -171940\n",
      "INFO:lda:<610> log likelihood: -172233\n",
      "INFO:lda:<620> log likelihood: -172308\n",
      "INFO:lda:<630> log likelihood: -171985\n",
      "INFO:lda:<640> log likelihood: -172218\n",
      "INFO:lda:<650> log likelihood: -171839\n",
      "INFO:lda:<660> log likelihood: -171806\n",
      "INFO:lda:<670> log likelihood: -172205\n",
      "INFO:lda:<680> log likelihood: -171850\n",
      "INFO:lda:<690> log likelihood: -172019\n",
      "INFO:lda:<700> log likelihood: -171802\n",
      "INFO:lda:<710> log likelihood: -171862\n",
      "INFO:lda:<720> log likelihood: -171719\n",
      "INFO:lda:<730> log likelihood: -171905\n",
      "INFO:lda:<740> log likelihood: -172217\n",
      "INFO:lda:<750> log likelihood: -172040\n",
      "INFO:lda:<760> log likelihood: -171697\n",
      "INFO:lda:<770> log likelihood: -171801\n",
      "INFO:lda:<780> log likelihood: -171436\n",
      "INFO:lda:<790> log likelihood: -171802\n",
      "INFO:lda:<800> log likelihood: -172050\n",
      "INFO:lda:<810> log likelihood: -171480\n",
      "INFO:lda:<820> log likelihood: -171514\n",
      "INFO:lda:<830> log likelihood: -171784\n",
      "INFO:lda:<840> log likelihood: -171806\n",
      "INFO:lda:<850> log likelihood: -171723\n",
      "INFO:lda:<860> log likelihood: -171667\n",
      "INFO:lda:<870> log likelihood: -171503\n",
      "INFO:lda:<880> log likelihood: -172064\n",
      "INFO:lda:<890> log likelihood: -171753\n",
      "INFO:lda:<900> log likelihood: -172080\n",
      "INFO:lda:<910> log likelihood: -171900\n",
      "INFO:lda:<920> log likelihood: -171527\n",
      "INFO:lda:<930> log likelihood: -171569\n",
      "INFO:lda:<940> log likelihood: -171644\n",
      "INFO:lda:<950> log likelihood: -171509\n",
      "INFO:lda:<960> log likelihood: -171471\n",
      "INFO:lda:<970> log likelihood: -171906\n",
      "INFO:lda:<980> log likelihood: -171562\n",
      "INFO:lda:<990> log likelihood: -171815\n",
      "INFO:lda:<1000> log likelihood: -171514\n",
      "INFO:lda:<1010> log likelihood: -171455\n",
      "INFO:lda:<1020> log likelihood: -171403\n",
      "INFO:lda:<1030> log likelihood: -171730\n",
      "INFO:lda:<1040> log likelihood: -171789\n",
      "INFO:lda:<1050> log likelihood: -171747\n",
      "INFO:lda:<1060> log likelihood: -171459\n",
      "INFO:lda:<1070> log likelihood: -171238\n",
      "INFO:lda:<1080> log likelihood: -171732\n",
      "INFO:lda:<1090> log likelihood: -171786\n",
      "INFO:lda:<1100> log likelihood: -171633\n",
      "INFO:lda:<1110> log likelihood: -171547\n",
      "INFO:lda:<1120> log likelihood: -171397\n",
      "INFO:lda:<1130> log likelihood: -171711\n",
      "INFO:lda:<1140> log likelihood: -171352\n",
      "INFO:lda:<1150> log likelihood: -171487\n",
      "INFO:lda:<1160> log likelihood: -171516\n",
      "INFO:lda:<1170> log likelihood: -171740\n",
      "INFO:lda:<1180> log likelihood: -171810\n",
      "INFO:lda:<1190> log likelihood: -171391\n",
      "INFO:lda:<1200> log likelihood: -171616\n",
      "INFO:lda:<1210> log likelihood: -171600\n",
      "INFO:lda:<1220> log likelihood: -171667\n",
      "INFO:lda:<1230> log likelihood: -171789\n",
      "INFO:lda:<1240> log likelihood: -171365\n",
      "INFO:lda:<1250> log likelihood: -171284\n",
      "INFO:lda:<1260> log likelihood: -171714\n",
      "INFO:lda:<1270> log likelihood: -171580\n",
      "INFO:lda:<1280> log likelihood: -171530\n",
      "INFO:lda:<1290> log likelihood: -171518\n",
      "INFO:lda:<1300> log likelihood: -171557\n",
      "INFO:lda:<1310> log likelihood: -171499\n",
      "INFO:lda:<1320> log likelihood: -171342\n",
      "INFO:lda:<1330> log likelihood: -171942\n",
      "INFO:lda:<1340> log likelihood: -171732\n",
      "INFO:lda:<1350> log likelihood: -171769\n",
      "INFO:lda:<1360> log likelihood: -171388\n",
      "INFO:lda:<1370> log likelihood: -171455\n",
      "INFO:lda:<1380> log likelihood: -171708\n",
      "INFO:lda:<1390> log likelihood: -171415\n",
      "INFO:lda:<1400> log likelihood: -171779\n",
      "INFO:lda:<1410> log likelihood: -171470\n",
      "INFO:lda:<1420> log likelihood: -171454\n",
      "INFO:lda:<1430> log likelihood: -171705\n",
      "INFO:lda:<1440> log likelihood: -171661\n",
      "INFO:lda:<1450> log likelihood: -171414\n",
      "INFO:lda:<1460> log likelihood: -171507\n",
      "INFO:lda:<1470> log likelihood: -171369\n",
      "INFO:lda:<1480> log likelihood: -171702\n",
      "INFO:lda:<1490> log likelihood: -171665\n",
      "INFO:lda:<1500> log likelihood: -171636\n",
      "INFO:lda:<1510> log likelihood: -171768\n",
      "INFO:lda:<1520> log likelihood: -171420\n",
      "INFO:lda:<1530> log likelihood: -171649\n",
      "INFO:lda:<1540> log likelihood: -171567\n",
      "INFO:lda:<1550> log likelihood: -171449\n",
      "INFO:lda:<1560> log likelihood: -171472\n",
      "INFO:lda:<1570> log likelihood: -171537\n",
      "INFO:lda:<1580> log likelihood: -171660\n",
      "INFO:lda:<1590> log likelihood: -171427\n",
      "INFO:lda:<1600> log likelihood: -171237\n",
      "INFO:lda:<1610> log likelihood: -171410\n",
      "INFO:lda:<1620> log likelihood: -171232\n",
      "INFO:lda:<1630> log likelihood: -171785\n",
      "INFO:lda:<1640> log likelihood: -171310\n",
      "INFO:lda:<1650> log likelihood: -171626\n",
      "INFO:lda:<1660> log likelihood: -171150\n",
      "INFO:lda:<1670> log likelihood: -171265\n",
      "INFO:lda:<1680> log likelihood: -171609\n",
      "INFO:lda:<1690> log likelihood: -171390\n",
      "INFO:lda:<1700> log likelihood: -171647\n",
      "INFO:lda:<1710> log likelihood: -171581\n",
      "INFO:lda:<1720> log likelihood: -171284\n",
      "INFO:lda:<1730> log likelihood: -171345\n",
      "INFO:lda:<1740> log likelihood: -171576\n",
      "INFO:lda:<1750> log likelihood: -171547\n",
      "INFO:lda:<1760> log likelihood: -171940\n",
      "INFO:lda:<1770> log likelihood: -171818\n",
      "INFO:lda:<1780> log likelihood: -171457\n",
      "INFO:lda:<1790> log likelihood: -171480\n",
      "INFO:lda:<1800> log likelihood: -171350\n",
      "INFO:lda:<1810> log likelihood: -171676\n",
      "INFO:lda:<1820> log likelihood: -171510\n",
      "INFO:lda:<1830> log likelihood: -171443\n",
      "INFO:lda:<1840> log likelihood: -171178\n",
      "INFO:lda:<1850> log likelihood: -171010\n",
      "INFO:lda:<1860> log likelihood: -171363\n",
      "INFO:lda:<1870> log likelihood: -171237\n",
      "INFO:lda:<1880> log likelihood: -171362\n",
      "INFO:lda:<1890> log likelihood: -171474\n",
      "INFO:lda:<1900> log likelihood: -171592\n",
      "INFO:lda:<1910> log likelihood: -171556\n",
      "INFO:lda:<1920> log likelihood: -171504\n",
      "INFO:lda:<1930> log likelihood: -171480\n",
      "INFO:lda:<1940> log likelihood: -171565\n",
      "INFO:lda:<1950> log likelihood: -171564\n",
      "INFO:lda:<1960> log likelihood: -171646\n",
      "INFO:lda:<1970> log likelihood: -171244\n",
      "INFO:lda:<1980> log likelihood: -171694\n",
      "INFO:lda:<1990> log likelihood: -171282\n",
      "INFO:lda:<1999> log likelihood: -171594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Word Distribution\n",
      "Topic 0: seemed crowd relaxing places restaurants machine spent environment similar separate\n",
      "Topic 1: reviews seated immediately parking waited absolutely pulled opened friday north\n",
      "Topic 2: sunday dish review tasting group decor staff etc friday stars\n",
      "Topic 3: audi dealership manager prius visits continued toyota appointment repairs maintenance\n",
      "Topic 4: burrito tacos taco chips asada carne rice meal vegas tasty\n",
      "Topic 5: breakfast sandwich sandwiches salad lounge panini hidden airport phoenix salads\n",
      "Topic 6: sunday stars wooden easily dhote corkage presentation fees towards frites\n",
      "Topic 7: dosa indian sambar dosas buffet masala bhavan quality thali rava\n",
      "Topic 8: building margaritas vibe rare tons setting flavored entry filling public\n",
      "Topic 9: und das ist der fr zimmer aber mile sich auch\n",
      "Topic 10: including multiple grab consider patio tight establishment greet venue learning\n",
      "Topic 11: sushi restaurant places crab madison employees tuna avocado restaurants oakville\n",
      "Topic 12: burgers onion add owner z places par cheese poutine bbq\n",
      "Topic 13: cheese polenta lamb restaurant cooked frites goat shared tartare served\n",
      "Topic 14: extra addition completely extremely stars yelp absolutely beyond detail mention\n",
      "Topic 15: prices portions vegetarian options changs lettuce weve pf takeout returning\n",
      "Topic 16: closed quotes en un di ton chose dates boutiques une\n",
      "Topic 17: spicy rice amount portions serving ambiance serve seats california restaurant\n",
      "Topic 18: brunch eggs toast oz saturday bacon crispy rosti berries anglaise\n",
      "Topic 19: options noticed entire pricing plenty closer spent explain unit treat\n",
      "Topic 20: overall server tip bill offered charged saturday traditional mix dishes\n",
      "Topic 21: theyre hockey visiting followed informed version market blonde familiar placed\n",
      "Topic 22: ashley plane pilot scottsdale photos christmas route continue moments landing\n",
      "Topic 23: regular paying priced selection despite restaurants honestly overall rate ambience\n",
      "Topic 24: jeremy av professional company dylan install theater installed installation equipment\n",
      "Topic 25: order meal waitress dining servers bartender tables chef menus handed\n",
      "Topic 26: served ordered dessert waiter beef appetizer entrees pasta meal potatoes\n",
      "Topic 27: location walking wifi staff hotel station edinburgh breakfast motel bathroom\n",
      "Topic 28: visit less meals certainly quickly selections visiting apparently picked listed\n",
      "Topic 29: owner customer business stars several overall gave lassi speak rating\n",
      "Topic 30: order server average tomato seemed restaurant requested greeted equally mushrooms\n",
      "Topic 31: highly staff fact quality spots based possible demi weekly products\n",
      "Topic 32: tasty decor seating welcoming glass eaten absolutely heavy lighting minimal\n",
      "Topic 33: upon etc opening sized shoppers samples ten anywhere theyve arriving\n",
      "Topic 34: pei wei restaurant asian location restaurants pf overall tofu panda\n",
      "Topic 35: muramoto sake meal tasty dishes tapas fusion cod dessert although\n",
      "Topic 36: customers customer manager received although gave explained replaced treated serve\n",
      "Topic 37: shopping shops shoppes mall palazzo vegas canal strip york outdoor\n",
      "Topic 38: order rice beef spicy ordered dish thai slices veggies sample\n",
      "Topic 39: quality ingredients value prepared generally tax staff sides general friday\n",
      "Topic 40: salad ordered tasted cheese dressing olive priced plate expectations sold\n",
      "Topic 41: plate scallops panna cotta creamy dessert braised servers bean reservations\n",
      "Topic 42: jamaican curry expecting patty krust patties caribbean ordered beef charlotte\n",
      "Topic 43: restaurant staff tables visit extremely atmosphere packed style quickly plates\n",
      "Topic 44: nail salon pedicure gel appointment manicure chairs polish pedi pedicures\n",
      "Topic 45: mussels seafood cooked ordered mussel portion chips scallops edinburgh quality\n",
      "Topic 46: indian lamb pizza kabobs naan kabob combination desi halal shish\n",
      "Topic 47: tea teas cafe selection te atmosphere pittsburgh study leaf cheese\n",
      "Topic 48: review simple theyre showed due chose expected style spoke walls\n",
      "Topic 49: beers brewery staff selection atmosphere ipa patio groupon craft nines\n"
     ]
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=50, n_iter=2000, random_state=1)\n",
    "model.fit(dtm)\n",
    "doc_topic = model.doc_topic_\n",
    "\n",
    "# print doc_topic probability distribution\n",
    "# for i, doc_dist in enumerate(doc_topic):\n",
    "#     print(\"Doc \", i)\n",
    "#     print(doc_dist)\n",
    "#     # for j,topic in enumerate(doc_dist):\n",
    "#     #     print \"Topic \",j,\" = \",topic\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "print(\"Topic Word Distribution\")\n",
    "\n",
    "# print topic_word ditribution\n",
    "n_top_words = 10\n",
    "# for i, word_dist in enumerate(topic_word):\n",
    "#     topic_words = np.array(vocab)[np.argsort(word_dist)][:-(n_top_words + 1):-1]\n",
    "#     word_dist_sorted = sorted(word_dist,reverse=True)\n",
    "#     print(\"Topic \", i)\n",
    "#     for j,words in enumerate(topic_words):\n",
    "#         print(words,\" = \",word_dist_sorted[j])\n",
    "#     # print \"Topic \", i,\" \", word_dist,\"\\n\"\n",
    "    \n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
